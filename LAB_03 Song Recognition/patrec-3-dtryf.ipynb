{"cells":[{"metadata":{"_uuid":"46bcadaf-f684-4bd2-aa1d-074036b46b83","_cell_guid":"7d964b91-835d-499e-9174-90479cd7eb9a","trusted":true},"cell_type":"code","source":"# %% [markdown]\n# # Αναγνώριση Είδους και Εξαγωγή Συναισθήματος από Μουσική\n\n# %% [code]\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code]\nimport librosa.display\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport os \n\nimport copy\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader\nimport re\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import TensorDataset\n\n# %% [code]\n# Keep all the train_labels information\ntrain_labels = pd.read_csv('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train_labels.txt')\n\n# List with just the label \ntrain_label=[]\nfor i in range(train_labels.shape[0]):\n    train_label.append(str(train_labels.iloc[i]).split('\\\\')[1].split(' ')[4])\n\ndef find_music_gender(num):\n    track1_pos = train_label.index(num +'.fused.full.npy.gz')\n    label1 = str(train_labels.iloc[track1_pos]).split('\\\\')[2].split(' ')[0][1:-6]\n    return label1\n    #print('label1:',label1, '\\nlabel2:',label2)\n\n# %% [markdown]\n# > # Step 1 [a-c]\n\n# %% [code]\nspec1 = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train/10079.fused.full.npy')\nprint('spec shape:',spec1.shape, 'Music Gender:', find_music_gender('10079'))\n\nspec2 = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train/10033.fused.full.npy')\nprint('spec shape:',spec2.shape, 'Music Gender:', find_music_gender('10033'))\n\n\nmel1, spec1 = spec1[:128], spec1[128:]\nmel2, spec2 = spec2[:128], spec2[128:]\n\n\n# PLOTS \nplt.figure(figsize = (10,6))\n\nplt.subplot(2, 1, 1)\nlibrosa.display.specshow(mel1, y_axis='linear')\nplt.colorbar(format='%+1.0f dB')\nplt.title(find_music_gender('10079'))\n\nplt.subplot(2, 1, 2)\nlibrosa.display.specshow(mel2, y_axis='linear')\nplt.colorbar(format='%+1.0f dB')\nplt.title(find_music_gender('10033'))\n\n# %% [markdown]\n# # Step 2 [a-b]\n\n# %% [code]\nspec1_beat = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train/10079.fused.full.npy')\nprint('spec shape:',spec1_beat.shape, 'Music Gender:', find_music_gender('10079'))\n\nspec2_beat = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train/10033.fused.full.npy')\nprint('spec shape:',spec2_beat.shape, 'Music Gender:', find_music_gender('10033'))\n\nmel1_beat, spec1_beat = spec1_beat[:128], spec1_beat[128:]\nmel2_beat, spec2_beat = spec2_beat[:128], spec2_beat[128:]\n\n\nplt.figure(figsize = (10,6))\n\nplt.subplot(2, 1, 1)\nlibrosa.display.specshow(mel1_beat, y_axis='linear')\nplt.colorbar(format='%+1.0f dB')\nplt.title('Beat ' +find_music_gender('10079'))\n\nplt.subplot(2, 1, 2)\nlibrosa.display.specshow(mel2_beat, y_axis='linear')\nplt.colorbar(format='%+1.0f dB')\nplt.title('Beat ' +find_music_gender('10033'))\n\n# %% [markdown]\n# \n\n# %% [markdown]\n# # Step 3\n\n# %% [code]\nplt.figure(figsize = (20,10))\n\nplt.subplot(2, 2, 1)\nlibrosa.display.specshow(spec1, y_axis='chroma')\nplt.colorbar(format='%+1.0f dB')\nplt.title('Chromagrams '+find_music_gender('10079'))\n\nplt.subplot(2, 2, 2)\nlibrosa.display.specshow(spec2, y_axis='chroma')\nplt.colorbar(format='%+1.0f dB')\nplt.title('Chromagrams '+find_music_gender('10079'))\n\nplt.subplot(2, 2, 3)\nlibrosa.display.specshow(spec1_beat, y_axis='chroma')\nplt.colorbar(format='%+1.0f dB')\nplt.title('Chromagrams Beat ' +find_music_gender('10033'))\n\nplt.subplot(2, 2, 4)\nlibrosa.display.specshow(spec2_beat, y_axis='chroma')\nplt.colorbar(format='%+1.0f dB')\nplt.title('Chromagrams Beat ' +find_music_gender('10033'))\n\n# %% [markdown]\n# # Step 4 [a-c]\n\n# %% [code]\nimport numpy as np\nimport copy\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader\nimport re\n\n#split dataset --create train and validation set \ndef torch_train_val_split(\n        dataset, batch_train, batch_eval,\n        val_size=.2, shuffle=True, seed=None):\n    #20% for validation\n    \n    dataset_size = len(dataset)\n    indices = list(range(dataset_size)) #indices for train and val\n    val_split = int(np.floor(val_size * dataset_size))\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n    train_indices = indices[val_split:]\n    val_indices = indices[:val_split]\n\n    # Sampling elements using the above indices\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n    #we use the custom sampler\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_train,\n                              drop_last=True,\n                              sampler=train_sampler)\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_eval,\n                            drop_last=True,\n                            sampler=val_sampler)\n    return train_loader, val_loader\n\ndef read_fused_spectrogram(spectrogram_file):\n    spectrogram = np.load(spectrogram_file)\n    return spectrogram.T\n\ndef read_mel_spectrogram(spectrogram_file):\n    spectrogram = np.load(spectrogram_file)[:128]\n    return spectrogram.T\n\n    \ndef read_chromagram(spectrogram_file):\n    spectrogram = np.load(spectrogram_file)[128:]\n    return spectrogram.T\n\n\n#Padding: bring all samples to the same length\nclass PaddingTransform(object):\n    def __init__(self, max_length, padding_value=0):\n        self.max_length = max_length\n        self.padding_value = padding_value\n\n    def __call__(self, s):\n        if len(s) == self.max_length:\n            return s\n\n        if len(s) > self.max_length:\n            return s[:self.max_length]\n\n        if len(s) < self.max_length:\n            s1 = copy.deepcopy(s)\n            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n            s1 = np.vstack((s1, pad))\n            return s1\n\nclass SpectrogramDataset(Dataset):\n    def __init__(self, path, task=None, train=True, max_length=-1, read_spec_fn=read_mel_spectrogram):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n        self.files, labels = self.get_files_labels(self.index, task)\n        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        if isinstance(labels, (list, tuple)):\n            self.labels = np.array(np.array(labels).astype('float'))\n\n    def get_files_labels(self, txt, task):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split(',') for l in fd.readlines()[1:]]\n        files, labels = [], []\n        for l in lines:\n            if task=='valence':\n                label = l[1]\n            elif task=='energy':\n                label = l[2]\n            else:\n                label = l[3]\n            # Kaggle automatically unzips the npy.gz format so this hack is needed\n            _id = l[0]\n            npy_file = '{}.fused.full.npy'.format(_id)\n            files.append(npy_file)\n            labels.append(label)\n        return files, labels\n\n    def __getitem__(self, item):\n        # TODO: Inspect output and comment on how the output is formatted\n        l = min(self.lengths[item], self.max_length)\n        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n\n    def __len__(self): #returns lenght\n        return len(self.labels)\n\n# %% [code]\nimport numpy as np\nimport copy\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader\nimport re\n\n# New classes to be combined \nclass_mapping = {\n    'Rock': 'Rock',\n    'Psych-Rock': 'Rock',\n    'Indie-Rock': None,\n    'Post-Rock': 'Rock',\n    'Psych-Folk': 'Folk',\n    'Folk': 'Folk',\n    'Metal': 'Metal',\n    'Punk': 'Metal',\n    'Post-Punk': None,\n    'Trip-Hop': 'Trip-Hop',\n    'Pop': 'Pop',\n    'Electronic': 'Electronic',\n    'Hip-Hop': 'Hip-Hop',\n    'Classical': 'Classical',\n    'Blues': 'Blues',\n    'Chiptune': 'Electronic',\n    'Jazz': 'Jazz',\n    'Soundtrack': None,\n    'International': None,\n    'Old-Time': None\n}\n\n\n\nclass LabelTransformer(LabelEncoder):\n    def inverse(self, y):\n        try:\n            return super(LabelTransformer, self).inverse_transform(y)\n        except:\n            return super(LabelTransformer, self).inverse_transform([y])\n\n    def transform(self, y):\n        try:\n            return super(LabelTransformer, self).transform(y)\n        except:\n            return super(LabelTransformer, self).transform([y])\n\n\nclass SpectrogramDatasetCategory(Dataset):\n    def __init__(self, path, class_mapping=None, train=True, max_length=-1, read_spec_fn=read_mel_spectrogram):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n        self.files, labels = self.get_files_labels(self.index, class_mapping)\n        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        self.label_transformer = LabelTransformer()\n        if isinstance(labels, (list, tuple)):\n            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n\n    def get_files_labels(self, txt, class_mapping):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n        files, labels = [], []\n        for l in lines:\n            label = l[1]\n            if class_mapping:\n                label = class_mapping[l[1]]\n            if not label:\n                continue\n            # Kaggle automatically unzips the npy.gz format so this hack is needed\n            _id = l[0].split('.')[0]\n            npy_file = '{}.fused.full.npy'.format(_id)\n            files.append(npy_file)\n            labels.append(label)\n        return files, labels\n\n    def __getitem__(self, item):\n        # TODO: Inspect output and comment on how the output is formatted\n        l = min(self.lengths[item], self.max_length)\n        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n\n    def __len__(self):\n        return len(self.labels)\n\n# %% [code]\ntrain_labels=(pd.read_csv('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train_labels.txt',dtype='str'))\n\ntrain_labels_classes=[]\nfor i in range(train_labels.shape[0]):\n    train_labels_classes.append(str(train_labels.iloc[i]).split('\\\\')[2].split(' ')[0][1:-6])\n\nunique_classes_before =set(train_labels_classes)\nunique_classes_after =set(class_mapping.values())\nunique_classes_after.remove(None)\n\nsamplesPerClass_before = {s:0 for s in unique_classes_before}\nsamplesPerClass_after = {s:0 for s in unique_classes_after}\n\nfor i in train_labels_classes:\n    samplesPerClass_before[i]+=1\n    \n    if (class_mapping[i]):\n        samplesPerClass_after[class_mapping[i]]+=1\n\nprint('Before: ',samplesPerClass_before)\nprint('After: ',samplesPerClass_after)\n\n\nplt.figure(figsize=(10,20))\n\nplt.subplot(2, 1, 1)\n\nplt.bar(samplesPerClass_before.keys(), samplesPerClass_before.values(), color='green')\nplt.xticks(rotation=45)\nplt.title('Before Class Merge')\n\nplt.subplot(2, 1, 2)\nplt.bar(samplesPerClass_after.keys(), samplesPerClass_after.values(), color='red')\nplt.title('After Class Merge')\n\n# %% [markdown]\n# # Step 5 [a-d]\n\n# %% [code]\nclass BasicLSTM(nn.Module):\n    def __init__(self, input_dim, rnn_size, output_dim, num_layers,drop_prob=0.2, bidirectional=False):\n        super(BasicLSTM, self).__init__()\n        self.bidirectional = bidirectional\n        self.feature_size = rnn_size * 2 if self.bidirectional else rnn_size\n        self.direction = 2 if self.bidirectional else 1\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n        self.input_dim = input_dim\n        self.rnn_size = rnn_size\n        self.lstm = nn.LSTM(input_dim,self.rnn_size, num_layers, dropout=drop_prob, batch_first=True, bidirectional=self.bidirectional) # init LSTM\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(self.feature_size, output_dim) #fully Connected \n\n      \n          \n    def forward(self, x, lengths,hidden):\n        \"\"\" \n            x : 3D numpy array of dimension N x L x D\n                N: batch index\n                L: sequence index\n                D: feature index\n\n            lengths: N x 1\n         \"\"\"\n              \n        \n        # We need only the last output (last_timestep method)\n        lstm_out, hidden = self.lstm(x, hidden)\n        batch_size=x.shape[0]\n        lstm_out = lstm_out.contiguous().view(batch_size,-1, self.feature_size)\n        out = self.fc(lstm_out)\n        #Keep last output per sequence \n        last_outputs = self.last_timestep(out,lengths,self.bidirectional)\n        return last_outputs,hidden\n\n    def last_timestep(self, outputs, lengths, bidirectional=False):\n        \"\"\"\n            Returns the last output of the LSTM taking into account the zero padding\n        \"\"\"\n        if bidirectional:\n            forward, backward = self.split_directions(outputs)\n            last_forward = self.last_by_index(forward, lengths)\n            last_backward = backward[:, 0, :]\n            return torch.cat((last_forward, last_backward), dim=-1)\n  \n\n        else:\n            return self.last_by_index(outputs, lengths)\n\n    @staticmethod\n    def split_directions(outputs):\n        direction_size = int(outputs.size(-1) / 2)\n        forward = outputs[:, :, :direction_size]\n        backward = outputs[:, :, direction_size:]\n        return forward, backward\n\n    @staticmethod\n    def last_by_index(outputs, lengths):\n        # Index of the last output for each sequence.\n        new_lengths=np.array([length-1 for length in lengths],dtype='long')\n        idx = (torch.from_numpy(new_lengths)).view(-1, 1)\n        idx = idx.expand(outputs.size(0),outputs.size(2)).unsqueeze(1).to(device)\n        return outputs.gather(1, idx).squeeze().to(device)\n\n    def init_hidden(self, batch_size):\n        #initialize hidden size \n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.direction*self.num_layers, batch_size, self.rnn_size).zero_().to(device),\n                      weight.new(self.direction*self.num_layers, batch_size, self.rnn_size).zero_().to(device))\n        return hidden\n\n# %% [code]\n# Turn on GPU - Manually in this case for Kaggle\nis_cuda = torch.cuda.is_available()\n\nif is_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n    \ndevice = torch.device('cuda')\n\n# %% [code]\n# from:  https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n\ndef train_model(input_dim,output_dim,save_string,data_loader,val_loader=None,category=True,val=True):\n    batch_size=32\n    rnn_size = 128 #hidden input\n    num_layers = 2 \n\n    model = BasicLSTM(input_dim,rnn_size,output_dim,num_layers)\n    model.to(device)\n    lr=0.0001\n    if (category):\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=0.001)\n    epochs = 30\n    print_every = 200\n    valid_loss_min = np.Inf\n    loss_values_train = []\n    loss_values_val = []\n    model.train()\n    counter=0\n\n\n\n    for i in range(epochs):\n        running_loss_train=0.0\n        running_loss_val=0.0\n        h = model.init_hidden(batch_size)\n        train_losses=[]\n        for inputs, labels,lengths in data_loader:\n            inputs,labels,lengths= inputs.to(device), labels.to(device),lengths.to(device)\n            counter += 1\n            h = tuple([e.data for e in h])\n            model.zero_grad()\n            output, h = model(inputs.float(),lengths,h)\n            if (category):\n                loss = criterion(output.squeeze(), labels.long())\n            else:\n                loss = criterion(output.squeeze(), labels.float())\n            running_loss_train =+ loss.item() * batch_size\n            train_losses.append(loss.item())\n\n            loss.backward()\n            optimizer.step()\n            if (counter%print_every == 0 and val):\n                val_h = model.init_hidden(batch_size)\n                val_losses = []\n                model.eval()\n                for inp, lab,lens in val_loader:\n                    inp,lab,lens=inp.to(device),lab.to(device),lens.to(device)\n                    val_h = tuple([each.data for each in val_h])\n                    out, val_h = model(inp.float(),lens, val_h)\n                    val_loss = criterion(out.squeeze(), lab.long())\n                    val_losses.append(val_loss.item())\n                    #running_loss_val=+ val_loss.item() * batch_size\n                model.train()\n                #'./state_mel_beat_dict.pt'\n                if np.mean(val_losses) <= valid_loss_min:\n                    torch.save(model.state_dict(), save_string) # Save model params in Dict\n                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                    valid_loss_min = np.mean(val_losses)\n        if val:\n            model.eval()\n            val_h = model.init_hidden(batch_size) \n            validation_losses=[]\n            \n            for inp, lab,lens in val_loader:\n                inp,lab,lens=inp.to(device),lab.to(device),lens.to(device)\n                val_h = tuple([each.data for each in val_h])\n                out, val_h = model(inp.float(),lens, val_h)\n                val_loss = criterion(out.squeeze(), lab.long())\n                validation_losses.append(val_loss.item())\n                running_loss_val=+ val_loss.item() * batch_size\n\n            model.train()\n        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n                \"Step: {}...\".format(counter),\n                \"Train Loss: {:.6f}...\".format(loss.item()))\n        loss_values_train.append(np.mean(train_losses))\n        if (val):\n            loss_values_val.append(np.mean(validation_losses))    \n    plt.plot(range(epochs),loss_values_train)\n    if (val):\n        plt.plot(range(epochs),loss_values_val)\n    if(not val):\n        torch.save(model.state_dict(), save_string)\n\n# %% [markdown]\n# ## Create Datasets\n\n# %% [code]\n#5a \n\nmel_specs = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',\n         train=True,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntrain_loader_mel, val_loader_mel = torch_train_val_split(mel_specs, 32 ,32, val_size=.33)\ntest_loader_mel = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',\n         train=False,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntest_loader_mel = DataLoader(test_loader_mel,\n                              batch_size=5,\n                              drop_last=True)\n\n# %% [code]\ntrain_model(128,10,'./state_mel_dict.pt',train_loader_mel,val_loader_mel)\n\n# %% [code]\n#5b\nbeat_mel_specs = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',\n         train=True,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntrain_loader_beat_mel, val_loader_beat_mel = torch_train_val_split(beat_mel_specs, 32 ,32, val_size=.33)\ntest_loader_beat_mel = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',\n         train=False,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntest_loader_beat_mel = DataLoader(test_loader_beat_mel,\n                              batch_size=5,\n                              drop_last=True)\n\n# %% [code]\ntrain_model(128,10,'./state_mel_beat_dict.pt',train_loader_beat_mel,val_loader_beat_mel)\n\n# %% [code]\n#5c\nchroma = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',\n         train=True,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_chromagram)\ntrain_loader_chroma, val_loader_chroma = torch_train_val_split(chroma, 32 ,32, val_size=.33)\ntest_loader_chroma = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',\n         train=False,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_chromagram)\ntest_loader_chroma = DataLoader(test_loader_chroma,\n                              batch_size=5,\n                              drop_last=True)\n\n# %% [code]\ntrain_model(12,10,'./chroma_dict.pt',train_loader_chroma,val_loader_chroma)\n\n# %% [code]\n#5d\nfused = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',\n         train=True,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_fused_spectrogram)\ntrain_loader_fused, val_loader_fused = torch_train_val_split(fused, 32 ,32, val_size=.33)\ntest_loader_fused = SpectrogramDatasetCategory(\n         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',\n         train=False,\n         class_mapping=class_mapping,\n         max_length=-1,\n         read_spec_fn=read_fused_spectrogram)\ntest_loader_fused = DataLoader(test_loader_fused,\n                              batch_size=5,\n                              drop_last=True)\n\n# %% [code]\ntrain_model(140,'./fused_dict.pt',train_loader_fused,val_loader_fused)\n\n# %% [markdown]\n# # Step 6\n\n# %% [code]\ndef test(input_dim,save_file,test_loader):\n    model = BasicLSTM(input_dim,128,10,2)\n    model.to(device)\n    criterion=nn.CrossEntropyLoss()\n    model.load_state_dict(torch.load(save_file)) #Load System's params (dictionary)\n    batch_size=5\n    test_losses = []\n    num_correct = 0\n    h = model.init_hidden(batch_size)\n    correct=0\n    y_pred_test=[]\n    y_true=[]\n    model.eval()\n    for inputs, labels,lengths in test_loader:\n          inputs,labels,lengths= inputs.to(device), labels.to(device),lengths.to(device)\n          h = tuple([each.data for each in h])\n          output, h = model(inputs.float(),lengths, h)\n          test_loss = criterion(output.squeeze(), labels.long())\n          test_losses.append(test_loss.item())\n          pred = output.data.max(1)[1]  # get the index of the max log-probability\n          y_pred_test.append(pred.tolist())\n          y_true.append(labels.tolist())\n          correct += pred.eq(labels.data).sum()\n\n    print('\\nTest set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n              np.mean(test_losses), correct, len(test_loader.dataset),\n              100. * correct / (len(test_loader.dataset))))\n\n    #test the model using all the different metrics (precision,recall, F1 score for all the classes)\n    print(np.array(y_true).shape)\n    print(np.array(y_pred_test).shape)\n    from sklearn.metrics import classification_report\n    print(classification_report(np.array(y_true).flatten(),np.array(y_pred_test).flatten()))\n\n# %% [code]\n#spectograms as input\ntest(128,'./state_mel_dict.pt',test_loader_mel)\n\n# %% [code]\n#beat-synced spectograms as input\ntest(128,'./state_mel_beat_dict.pt',test_loader_beat_mel)\n\n# %% [code]\n#chromagrams as input\ntest(12,'./chroma_dict.pt',test_loader_chroma)\n\n# %% [code]\n#concatenated spectograms and chromograms as input\ntest(140,'./fused_dict.pt',test_loader_fused)\n\n# %% [markdown]\n# # Step 7\n\n# %% [code]\n#b\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n#CNN initinialization\nclass myCNN(nn.Module):\n    def __init__(self):\n        super(myCNN,self).__init__()\n        self._cnn_module = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4),\n            Flatten()\n        \n        )\n        self._fc_module = nn.Sequential(\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=10240, out_features=1)\n        )\n        #Weights Initialization\n        #Weights only from the linear and convolutional layer\n        \n    def forward(self, x):\n        x=x.transpose(1,2) # dims [batch_size,1,]\n        x=torch.unsqueeze(x,1)\n        for layer in self._cnn_module:\n            x = layer(x)\n        \n        for layer in self._fc_module:\n            x=layer(x)\n        return x\n    def initialize_weights(self,layer)->None:\n        if isinstance(layer, nn.Conv2d):\n            #weights based on Gaussian (mean=0, std = sqrt(2/#Neurons))\n            nn.init.kaiming_uniform_(layer.weight)\n        elif isinstance(layer, nn.Linear):\n            #we use xavier initialization\n            nn.init.xavier_uniform_(layer.weight)\n\n# %% [code]\n#c\ndef train_cnn(data_loader,save_string,epochs,category=False):\n    model_cnn = myCNN()\n    if category:\n        model_cnn._fc_module = nn.Sequential(\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=10240, out_features=10)\n        )\n    \n    batch_size=16\n    output_dim = 1\n\n    model_cnn.to(device)\n    lr=0.001\n    if category:\n        criterion=nn.CrossEntropyLoss()\n    else:\n        criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model_cnn.parameters(), lr=lr,weight_decay=0.0001)\n\n    train_loss_min = np.Inf\n    loss_values_train = []\n    loss_values_val = []\n    model_cnn.train()\n    counter=0\n    \n\n\n    model_cnn.train()    \n    for i in range(epochs):\n        running_loss_train=0.0\n        running_loss_val=0.0\n        train_losses=[]\n        for inputs, labels,lengths in data_loader:\n            inputs,labels,lengths= inputs.to(device), labels.to(device),lengths.to(device)\n            counter += 1\n            model_cnn.zero_grad()\n            output =  model_cnn(inputs.float())\n            if category:\n                loss = criterion(output.squeeze(), labels.long())\n            else:\n                loss = criterion(output.squeeze(), labels.float())\n            running_loss_train =+ loss.item() * batch_size\n            train_losses.append(loss.item())\n\n            loss.backward()\n\n            optimizer.step()\n\n\n        model_cnn.eval()\n        model_cnn.train()\n        print(\"Epoch: {}/{}...\".format(i+1, epochs),\n                \"Step: {}...\".format(counter),\n                \"Train Loss: {:.6f}...\".format(loss.item()))\n        loss_values_train.append(np.mean(train_losses))\n        if np.mean(train_losses)<train_loss_min:\n            torch.save(model_cnn.state_dict(),save_string)\n    plt.plot(range(epochs),loss_values_train)\n    #torch.save(model_cnn.state_dict(),save_string)\n\n# %% [code]\ntrain_cnn(train_loader_mel,'state_mel_cnn.pt',epochs=45,category=True)\n\n# %% [code]\n#d\nfrom sklearn.metrics import classification_report\n\ndef cnn_test(save_file,test_loader):\n    criterion=nn.CrossEntropyLoss()\n    model_cnn = myCNN()\n    model_cnn._fc_module = nn.Sequential(\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=10240, out_features=10)\n    )\n    model_cnn.to(device)\n    model_cnn.load_state_dict(torch.load(save_file))\n    batch_size=5\n    test_losses = []\n    num_correct = 0\n    correct=0\n    y_pred_test=[]\n    y_true=[]\n    model_cnn.eval()\n    for inputs, labels,lengths in test_loader:\n          inputs,labels,lengths= inputs.to(device), labels.to(device),lengths.to(device)\n          output = model_cnn(inputs.float())\n          test_loss = criterion(output.squeeze(), labels.long())\n          test_losses.append(test_loss.item())\n          pred = output.data.max(1)[1]  # get the index of the max log-probability\n          y_pred_test.append(pred.tolist())\n          y_true.append(labels.tolist())\n          correct += pred.eq(labels.data).sum()\n\n    print('\\nTest set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n              np.mean(test_losses), correct, len(test_loader.dataset),\n              100. * correct / (len(test_loader.dataset))))\n    print(classification_report(np.array(y_true).flatten(),np.array(y_pred_test).flatten()))\ncnn_test('./state_mel_cnn.pt',test_loader_mel)\n\n# %% [markdown]\n# # Step 8\n\n# %% [code]\n#8a\n#create a pytorch dataset for the mel spectogram\nmel_specs_energy = SpectrogramDataset(\n         '../input/patreco3-multitask-affective-music/data/multitask_dataset_beat/',\n         train=True,\n         task='energy',\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntrain_loader_energy, test_loader_energy= torch_train_val_split(mel_specs_energy, 32 ,32, val_size= 0.2 )\n\nmel_specs_valence = SpectrogramDataset(\n         '../input/patreco3-multitask-affective-music/data/multitask_dataset_beat/',\n         train=True,\n         task='valence',\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntrain_loader_valence, test_loader_valence = torch_train_val_split(mel_specs_valence, 32 ,32, val_size= 0.2 )\n\nmel_specs_danceability = SpectrogramDataset(\n         '../input/patreco3-multitask-affective-music/data/multitask_dataset_beat/',\n         train=True,\n         task='danceability',\n         max_length=-1,\n         read_spec_fn=read_mel_spectrogram)\ntrain_loader_danceability, test_loader_danceability = torch_train_val_split(mel_specs_danceability, 32 ,32, val_size= 0.2 )\n\n# %% [code]\n#8b\ntrain_cnn(train_loader_energy,'state_mel_energy_cnn.pt',epochs=30)\n\n# %% [code]\n#8c\ntrain_cnn(train_loader_valence,'state_mel_valence_cnn.pt',epochs=36)\n\n# %% [code]\n#8d\ntrain_cnn(train_loader_danceability,'state_mel_danceability_cnn.pt',epochs=30)\n\n# %% [code]\n#8e\nimport scipy.stats\n\n\ndef lstm_model_multitask(test_loader,save_file):\n    model_lstm = BasicLSTM (128,128,1,2)\n    model_lstm.to(device)\n    model_lstm.load_state_dict(torch.load(save_file))\n    batch_size=32\n    h = model_lstm.init_hidden(batch_size)\n    test_losses = []\n    y_pred_test=[]\n    y_true=[]\n    model_lstm.eval()\n    for inputs, labels,lengths in test_loader:\n          inputs,labels,lengths= inputs.to(device), labels.to(device),lengths.to(device)\n          h = tuple([each.data for each in h])\n          output, h = model_lstm(inputs.float(),lengths, h)\n          y_pred_test.append(output.data.tolist())\n          y_true.append(labels.tolist())\n    rho= scipy.stats.spearmanr(np.array(y_true).flatten(),np.array(y_pred_test).flatten()).correlation\n    print('\\nTest set: Spearman Correlation: {:.6f} \\n'.format(rho))\n    return rho\n\nr_energy=lstm_model_multitask(test_loader_energy,'./state_mel_energy_multi.pt')\nr_valence=lstm_model_multitask(test_loader_valence,'./state_mel_valence_multi.pt')\nr_danceability = lstm_model_multitask(test_loader_danceability,'./state_mel_danceability_multi.pt')\nprint((r_energy+r_valence+r_danceability)/3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}